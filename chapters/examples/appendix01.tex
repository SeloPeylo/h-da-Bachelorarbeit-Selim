\chapter{Docker Images für Jenkins Agenten}
\label{appendix:docker}
Dieses Kapitel wurde während der Ausgangssituation am Anfang der Thesis vorgesehen und sollte die Ergebnisse aus der Erstellung von Docker-Images für die \ac{SEU} eines Kreditinstituts technisch darstellen. Mit der Strategie das Problem von unten hochzuklettern und von oben deduktiv vorzugehen führte unvermeidlich zu einer Divergenz der ursprünglich vorgesehenen Lösungen der Forschungsfragen. Aus der Durchleuchtung des Problems von weiteren Blickwinkeln \ref{ch:background} wurde klar, dass teilweise Symptome des Problems behandelt wurden. Daher macht es mehr Sinn Prinzipien wie die Standardisierung zu evaluieren und auf bekannte Lösungen zu referenzieren. Die vorgenommenen Anpassungen sind durchaus für die Migration der \ac{SEU} auf eine Cloud-Plattform in einem Kreditinstitut interessant, können jedoch aufgrund der pandemiebedingten Remotearbeit nicht für den Anwendungsfall evaluiert werden. Daher entschloss sich der Autor die Ergebnisse und vorgenommenen Themen als Anhang beizufügen, da die Beschäftigung mit diesem Thema und ein daraus resultierender erster Prototyp zur Lösungsfindung relevant war.


\section{Entwicklung von Docker-Images für Jenkins Agenten}

Während der Entwicklung eines Kubernetes Clusters für die \ac{SEU} gibt es viele Parameter, die sich ständig verändern können. Beispielsweise kann die Entscheidung für oder gegen eine Public-Cloud offen stehen oder es stehen Anpassungen offen. Daher kann nicht immer auf der größten Ebene angefangen werden.

Ein Ansatz für das Re-Design von Anwendungen ist die Entkernung und Modularisierung von Systemen \cite{Bussmann2006}.
Für einen gemeinsamen Nenner sollte die \ac{SEU} anhand ihrer wesentlichen Funktionen aufgeteilt werden \ref{jenkins:skalierung}. Dadurch kann anschließend die Entwicklung ihrer Komponenten parallel durchgeführt werden. 

\paragraph{Kleinster gemeinsamer Nenner der SEU}
Der kleinste gemeinsame Nenner der \ac{SEU} sind die einzelnen Stages aus den Pipelines, die nach dem Regelprozess für die Softwareentwicklung entwickelt wurden und die einzelnen Schritte aus dem Prozess darstellen.

Der größte Skalierungsbedarf besteht im Build-Stage, da sie sehr Rechen- und Speicherintensiv ist. Diese Komponente sollte als erstes optimiert werden.
Für die Build-Jobs gemäß der Pipeline ist die Plattform, in der sie sich befindet in erster Linie nicht relevant. 

\paragraph{Worauf es ankommt}
Die Build-Jobs werden in der Regel verteilt auf Agenten. Wichtig hierbei ist ihre Kommunikation zu den beteiligten Systemen. Beteiligte Systeme sind:
\begin{itemize}
    \item Versionsverwaltung: Sie verwaltet den Quellcode, der für den Build von ihr bezogen wird
    \item Artefaktrepository: Sie ist die Quelle, aus der Softwareartefakte bezogen werden und sichert und verfolgt die Ergebnisartefakte
    \item Integrationsplattform: Sie delegiert die Build-Jobs, welche die praktische Ausführung der Pipelines definieren
\end{itemize}

Agenten sind self-contained und werden in der Regel auf hierfür konfigurierten Maschinen ausgeführt. Daher kann bereits mit der Entwicklung von Images für die Agenten begonnen werden, während wichtige Aspekte des Gesamtsystems noch geklärt werden.

\paragraph{Vorteile von Container}
Container bieten klare Vorteile gegenüber klassischen VMs. 

\paragraph{Use-Case}
Die Images werden zur Erzeugung von Containern innerhalb eines Kubernetes Clusters für die Skalierung einer Komponente der Integrationsplattform Jenkins verwendet. Diese werden erzeugt, für den Einsatz als Agent innerhalb einer Master-Slave Konfiguration von Jenkins für verteilte Builds. Diese Agenten sind für die Ausführung eines Java Builds mit dem Build-Management Tool Maven zuständig.

\paragraph{Anforderungen}
\begin{itemize}
    \item Aufgrund von hohen Gebühren für cloud-out-traffic soll die Image möglichst klein sein
    \item Die Umgebung soll \ac{JDK11} und Maven enthalten und entsprechend konfiguriert werden
    \item Beim Bauen der Image sollen alle Ressourcen intern über eine private Artifactory-Instanz bezogen werden
    \item Zum Erstellen eines Builds soll Maven mit Artifactory kommunizieren und Daten übertragen können
    \item Zur Evaluation der Umgebung soll Maven einen erfolgreichen Build zu einem Testprojekt erstellen
\end{itemize}



\subsection{Auswahl eines Basis-Image}
Der erste Schritt in einem Dockerfile ist das Festlegen eines Basis-Images, aus welcher die Umgebung konfiguriert werden soll. Hierzu wird am Anfang der Dockerfile \lstinline{FROM image:tag} definiert. Theoretisch kann eine Image auch aus einer leeren Umgebung \lstinline{FROM scratch} erzeugt werden. Das ist jedoch unüblich, selbst bei Containern für embedded Systeme.

Hierzu gibt es im Docker-Hub öffentliche Images, die umkonfiguriert werden können. Die offizielle Maven image \lstinline{maven:3.6.3-jdk-11}, die Maven und JDK11 bereits installiert hat, könnte beispielsweise für eine einfache Anwendung verwendet werden. Für den Use-Case innerhalb der Continuous Integration Plattform können die potenziellen Basis-Images aus dem Dockerhub zwischen folgenden Stufen in ihrer Ausstattung unterschieden werden:
\begin{enumerate}
    \item Betriebssystem: \lstinline{ubuntu, debian, alpine}
    \item SDK: \lstinline{jdk11, openjdk11, adoptopenjdk11, amazoncoretto11}
    \item Build-Management: \lstinline{maven}
\end{enumerate}
Bei einer hohen Ausstattungsstufe besteht jedoch eine hohe Abhängigkeit von einer bestimmten Image, die wiederrum auf niedrigeren Stufen basiert. Für eine möglichst hohe Nachvollziehbarkeit wäre es sinnvoll eine möglichst niedrige Stufe zu wählen und die Umgebung innerhalb der eigenen Infrastruktur zu konfigurieren. Für eine möglichst hohe Effizienz der Container sind jedoch umfangreiche Kenntnisse über Betriebssysteme erforderlich. Diese fällt je nach Umfang in den Tätigkeitsbereich eines Systementwicklers.

\paragraph{Vorteile von selbst gebauten Images}
Für das Gesamtbild der Skalierbarkeit ist es jedoch sinnvoll das Ganze mit einer hohen Kompetenz anzugehen. Möglichst minimale und effiziente Container-Umgebungen wirken sich kumulativ auf den Ressourcenbedarf des ganzen Clusters aus. Bei einer Public-Cloud werden üblicherweise die genutzten Ressourcen mit einem Studensatz abgerechnet. Ein weiterer Vorteil entsteht durch das Prinzip des Layerings bei Container-Umgebungen. Auf einer eigenen Image können weitere Images basieren. Hierdurch muss lediglich eine Basis-Image entwickelt werden, in welchem nur die essentiellen Anpassungen vorgenommen werden, mit dem Ziel ein intern evaluiertes Betriebssystem bereitzustellen. Basierend hierauf können unabhängig voneinander Images aufbauen mit den entsprechenden Tools für die unterschiedlichen Programmiersprachen. Dadurch entsteht eine hohe Synergie und Flexibilität für die Systementwicklung und den Betrieb der Systeme.

\paragraph{Minimale Images}
Eine Best-Practice innerhalb der Entwicklung von Docker-Images ist ihre Größe so niedrig wie möglich zu halten. 

Die Linux-Image \lstinline{alpine} ist hierfür als Basis-Image besonders beliebt, da sie mit einer Größe von nur \lstinline{5 MB} eine Linux-Umgebung abbildet und eine Package-Repository besitzt \cite{docker-alpine}. Alpine benutzt jedoch nicht die C Standardlibrary \lstinline{glibc}, sondern \lstinline{musl libc} \cite{alpine-about}. Daher sollte die Nutzung gerade zum Kompilieren von Quellcode abgewägt werden.
Auch mit den offiziellen Docker-Images von Ubuntu und Debian kommt man auch mit entsprechenden Praktiken auf eine niedrige Größe. Für den Anwendungsfall innerhalb der \ac{CI} Plattform ist es wichtig eine kleinstmögliche Java Installation durchzuführen. Hierfür gibt es entsprechende \lstinline{slim} Editionen der JDK. 

Ein Vergleich zwischen der normalen \lstinline{openjdk:11-jdk} mit der \lstinline{-slim}\footnote{Vgl. Images aus index.docker.io}:
\begin{verbatim}
openjdk:11-jdk
SIZE 306.67 MB
sha256:9efbdac6886418e7c473ee4d59ff728d029fad773364cb671794333d-
d16e4158

openjdk:11-jdk-slim
SIZE 216.36 MB
sha256:a67455311b2982c4cb795c8a38d55cf17e840c8fdb9ca82d14cd38d8-
43e24111
\end{verbatim}

Beide Images basieren auf \lstinline{debian:buster}, wobei die zweite Image auf \lstinline{debian:buster-slim} basiert.

Ein vergleich zwischen \lstinline{alpine:3.12} und \lstinline{debian:buster-slim}
\begin{verbatim}
alpine:3.12
SIZE 2.3 MB
sha256:c929c5ca1d3f793bfdd2c6d6d9210e2530f1184c0f488f514f1bb808-
0bb1e82b

debian:buster-slim
SIZE 26.46 MB
sha256:9f8288b62780eaa2ee6341c6f8632990eae83d9b1caac9d0e4545e6f-
c8ce5e6d
\end{verbatim}

Auf die absolute Größe kommt es nicht an. Viel wichtiger ist es bei der Konfiguration der Images sich genau mit den Layers zu befassen. Jedes Befehl in der Dockerfile erzeugt einen neuen Layer. Das Löschen von Dateien wirkt sich nicht auf die vorigen Layer aus. Daher sollten Befehle verkettet als ein Befehl ausgeführt werden und möglichst wenige Daten erzeugt werden. 
\medskip
\\
Aufgrund der geringen Auswirkung der absoluten Größe wird möglichst auf eine Standardumgebung als Basis-Image verwendet. Hierfür wurde \lstinline{debian:buster-slim} ausgewählt. Die Debian Image auf Dockerhub ist eine offizielle Image, die hauptsächtlich vom Debian Maintainer tianon gepflegt und veröffentlicht wird \cite{docker-debian-builds, docker-debian}.

\paragraph{Weitere Schritte}
\begin{itemize}
    \item Konfiguration der Images
    \item Kommunikation mit Artifactory
    \item Bezug von Build-Tools über Artifactory
    \item Installation von Java
    \item Cacerts
    \item Installation von Maven
    \item Konfiguration von Maven
    \item Einfügen eines Testjobs
    \item Bauen von Images
    \item Evaluation der Images
    \item Bereitstellung der Images
    \item Integration in Jenkins
    \item Agent über Secure Shell
    \item Agent über Java Network Launch Protocol
\end{itemize}